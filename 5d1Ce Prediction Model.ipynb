{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revised file was created on Mon Dec 30 14:00:00 2024\n",
    "\n",
    "Author: Nakyung Lee, University of Houston; Se√°n Kavanagh, University of Birmingham\n",
    "\n",
    "This code includes three different models.\n",
    "1. revised relative permittivity prediction\n",
    "2. revised centroide shift prediction\n",
    "3. 5d1 Ce prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptor for relative permittivity, centroid shift, and 5d1 Ce models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from scipy.spatial import ConvexHull, QhullError\n",
    "from pymatgen.core.composition import Composition\n",
    "from pymatgen.core.periodic_table import Species, Element\n",
    "from pymatgen.core.periodic_table import Specie\n",
    "from pymatgen.core.structure import Molecule\n",
    "from pymatgen.analysis.local_env import CrystalNN\n",
    "from pymatgen.analysis.ewald import EwaldSummation\n",
    "from pymatgen.analysis.structure_analyzer import SpacegroupAnalyzer\n",
    "from pymatgen.analysis.chemenv.coordination_environments.coordination_geometry_finder import LocalGeometryFinder\n",
    "from pymatgen.symmetry.analyzer import PointGroupAnalyzer\n",
    "from tqdm import tqdm\n",
    "from mp_api.client import MPRester\n",
    "from pymatgen.io.cif import CifParser\n",
    "from pymatgen.transformations.advanced_transformations import OrderDisorderedStructureTransformation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cation Site Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "polar_pointgroups = [\"1\",\"2\",\"m\",\"mm2\",\"3\",\"3m\",\"4\",\"4m\",\"6\",\"6mm\"] \n",
    "average_anion_polarizability = {\"F\": 0.634, \"Cl\": 2.2, \"Br\": 3.1, \"I\": 5, \"O\": 0.793, \"S\": 2.9, \"Se\": 3.8, \"N\": 1.1}\n",
    "cations_of_interest = [\"Na+\",\"K+\",\"Rb+\",\"Cs+\",\"Ca2+\",\"Sr2+\",\"Ba2+\",\"Y3+\",\"Ce4+\",\"La3+\",\"Pr3+\",\n",
    "                       \"Nd3+\",\"Sm3+\",\"Eu3+\",\"Gd3+\",\"Tb3+\",\"Dy3+\",\"Er3+\",\"Tm3+\",\"Yb3+\",\"Lu3+\"] \n",
    "anions_of_interest = [\"O2-\", \"F-\", \"Cl-\", \"Br-\", \"I-\", \"S2-\", \"N3-\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _custom_formatwarning(msg, *args, **kwargs):\n",
    "    return f\"{msg}\\n\"\n",
    "warnings.formatwarning = _custom_formatwarning\n",
    "\n",
    "def polyhedron_volume(vertices):\n",
    "    if len(set(tuple(point) for point in vertices)) < 3:\n",
    "        return 0\n",
    "    try:\n",
    "        hull = ConvexHull(vertices)\n",
    "        return hull.volume\n",
    "    except Exception as e:\n",
    "        return 0\n",
    "\n",
    "# Function to convert integer to Roman values, for extracting Shannon radii\n",
    "def roman(number):\n",
    "    num = [1, 4, 5, 9, 10, 40, 50, 90, 100, 400, 500, 900, 1000]\n",
    "    sym = [\"I\", \"IV\", \"V\", \"IX\", \"X\", \"XL\", \"L\", \"XC\", \"C\", \"CD\", \"D\", \"CM\", \"M\"]\n",
    "    i = 12\n",
    "    roman_string = \"\"\n",
    "\n",
    "    while number:\n",
    "        div = number // num[i]\n",
    "        number %= num[i]\n",
    "\n",
    "        while div:\n",
    "            roman_string += sym[i]\n",
    "            div -= 1\n",
    "        i -= 1\n",
    "\n",
    "    return roman_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CrystalNN(cation_anion=True)\n",
    "\n",
    "def get_polyhedron_dict(structure, site_index, cnn=cnn, dopant_species=\"Ce3+\", verbose=False):\n",
    "    structure_w_oxi = structure.copy()\n",
    "    structure_w_oxi.add_oxidation_state_by_guess(max_sites=-1)  # add oxidation states to structure\n",
    "\n",
    "    polyhedron_dict = {}\n",
    "\n",
    "    nn_info_dict = cnn.get_nn_info(structure_w_oxi, site_index)\n",
    "    polyhedron_dict[\"coordination_number\"] = cnn.get_cn(structure_w_oxi, site_index)\n",
    "\n",
    "    # get ionic radii of central atom and dopant:\n",
    "    cation_site = structure_w_oxi.sites[site_index]\n",
    "    cation_specie = cation_site.specie\n",
    "    try:\n",
    "        cation_ionic_radius = cation_specie.get_shannon_radius(\n",
    "            cn=roman(polyhedron_dict[\"coordination_number\"]), radius_type=\"ionic\"\n",
    "        )\n",
    "    except KeyError:\n",
    "        warnings.warn(\n",
    "            f\"No tabulated Shannon radius for {cation_specie} with coordination number {polyhedron_dict['coordination_number']}. Using CN-independent (average) ionic radius instead.\"\n",
    "        )\n",
    "        cation_ionic_radius = cation_site.specie.ionic_radii.get(\n",
    "            cation_specie.oxi_state, None\n",
    "        )\n",
    "    if cation_ionic_radius is None:\n",
    "        warnings.warn(\n",
    "            f\"No tabulated ionic radius for cation {cation_site.specie} with oxidation state {guessed_oxi_state:+d}. Using average ionic radius instead.\"\n",
    "        )\n",
    "        cation_ionic_radius = cation_site.specie.average_ionic_radius\n",
    "    dopant_site = structure_w_oxi.sites[site_index]\n",
    "    dopant_specie = Specie.from_str(dopant_species)\n",
    "    try:\n",
    "        dopant_ionic_radius = dopant_specie.get_shannon_radius(\n",
    "            cn=roman(polyhedron_dict[\"coordination_number\"]), radius_type=\"ionic\"\n",
    "        )\n",
    "    except KeyError:\n",
    "        warnings.warn(\n",
    "            f\"No tabulated Shannon radius for {dopant_specie} with coordination number {polyhedron_dict['coordination_number']}. Using CN-independent (average) ionic radius instead.\"\n",
    "        )\n",
    "        dopant_ionic_radius = dopant_site.specie.ionic_radii.get(\n",
    "            dopant_specie.oxi_state, None\n",
    "        )\n",
    "    if dopant_ionic_radius is None:\n",
    "        warnings.warn(\n",
    "            f\"No tabulated ionic radius for dopant {dopant_specie} with oxidation state {dopant_specie.oxi_state:+f}. Using average ionic radius instead.\"\n",
    "        )\n",
    "        dopant_ionic_radius = dopant_site.specie.average_ionic_radius\n",
    "\n",
    "    polyhedron_dict[\"cation_ionic_radius\"] = cation_ionic_radius  # Angstrom\n",
    "    polyhedron_dict[\"dopant_ionic_radius\"] = dopant_ionic_radius  # Angstrom\n",
    "    polyhedron_dict[\"delta_radius\"] = (\n",
    "        cation_ionic_radius - dopant_ionic_radius\n",
    "    )  # Angstrom\n",
    "\n",
    "    # get min, max and mean of metal-ligand bond lengths:\n",
    "    bond_lengths = []  # Bond Lengths\n",
    "    for i in nn_info_dict:\n",
    "        bond_lengths.append(\n",
    "            {\n",
    "                \"Element\": i[\"site\"].specie.as_dict()[\"element\"],\n",
    "                \"Distance\": f\"{i['site'].distance(cation_site):.3f}\",\n",
    "            }\n",
    "        )\n",
    "    bond_lengths.sort(key=lambda x: x[\"Distance\"])\n",
    "    polyhedron_dict[\"Avg_bond_length\"] = np.mean(\n",
    "        [float(bond_length[\"Distance\"]) for bond_length in bond_lengths]\n",
    "    )  # Angstrom\n",
    "\n",
    "    return polyhedron_dict\n",
    "\n",
    "lgf = LocalGeometryFinder()\n",
    "\n",
    "def get_chemenv_info(structure, isite):\n",
    "    lgf.setup_structure(structure=structure)\n",
    "    coord_envs = lgf.compute_coordination_environments(structure)\n",
    "    if len(coord_envs[isite]) > 1:\n",
    "        # choose the one with the higher ce_fraction:\n",
    "        coord_envs[isite] = sorted(\n",
    "            coord_envs[isite], key=lambda x: x[\"ce_fraction\"], reverse=True\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        chemenv_dict = {\n",
    "            #\"ce_symbol\": coord_envs[isite][0][\"ce_symbol\"],\n",
    "                        \"chemenv_CN\": coord_envs[isite][0][\"ce_symbol\"].split(\":\")[1]}\n",
    "    except IndexError:\n",
    "        chemenv_dict = {\"chemenv_CN\": None}\n",
    "\n",
    "    return chemenv_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crystal Structure Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_struc_dict(structure):\n",
    "    struc_dict = {}\n",
    "    sga = SpacegroupAnalyzer(structure)\n",
    "    conv_struc = sga.get_conventional_standard_structure()\n",
    "\n",
    "    struc_dict[\"SGR No.\"] = sga.get_space_group_number()\n",
    "    struc_dict[\"volume\"] = conv_struc.volume  # Angstrom^3\n",
    "    struc_dict[\"density\"] = conv_struc.density\n",
    "    struc_dict[\"volume_per_Z\"] = conv_struc.volume / conv_struc.composition\\\n",
    "        .get_reduced_composition_and_factor()[1]  # Angstrom^3\n",
    "    struc_dict[\"a\"] = conv_struc.lattice.a  # Angstrom\n",
    "    struc_dict[\"b\"] = conv_struc.lattice.b  # Angstrom\n",
    "    struc_dict[\"c\"] = conv_struc.lattice.c\n",
    "    struc_dict[\"alpha\"] = conv_struc.lattice.alpha\n",
    "    struc_dict[\"beta\"] = conv_struc.lattice.beta\n",
    "    struc_dict[\"gamma\"] = conv_struc.lattice.gamma  # degrees\n",
    "    struc_dict[\"inversion center\"] = sga.is_laue()\n",
    "    struc_dict[\"inversion center\"] = int(struc_dict[\"inversion center\"])\n",
    "    struc_dict[\"polar axis\"] = sga.get_point_group_symbol() in polar_pointgroups\n",
    "    struc_dict[\"polar axis\"] = int(struc_dict[\"polar axis\"])\n",
    "\n",
    "    struc_dict[\"a/b\"] = conv_struc.lattice.a / conv_struc.lattice.b\n",
    "    struc_dict[\"b/c\"] = conv_struc.lattice.b / conv_struc.lattice.c\n",
    "    struc_dict[\"c/a\"] = conv_struc.lattice.c / conv_struc.lattice.a\n",
    "    struc_dict[\"alpha/beta\"] = conv_struc.lattice.alpha / conv_struc.lattice.beta\n",
    "    struc_dict[\"beta/gamma\"] = conv_struc.lattice.beta / conv_struc.lattice.gamma\n",
    "    struc_dict[\"gamma/alpha\"] = conv_struc.lattice.gamma / conv_struc.lattice.alpha\n",
    "    struc_dict[\"volume_rp\"] = conv_struc.volume /10**3\n",
    "    struc_dict[\"volume_per_Z_rp\"] = conv_struc.volume / conv_struc.composition\\\n",
    "        .get_reduced_composition_and_factor()[1] / 10**3\n",
    "    struc_dict[\"volume_per_atom\"] = (conv_struc.volume / conv_struc.composition.num_atoms) / 10**3  # Angstrom^3\n",
    "\n",
    "    structure.add_oxidation_state_by_guess(max_sites=-1)\n",
    "    struc_dict[\"Avg. cation electronegativity\"] = np.mean([\n",
    "        Element(site.specie.element).X\n",
    "        for site in structure.sites\n",
    "        if site.specie.oxi_state >= 0\n",
    "    ])\n",
    "    struc_dict[\"Avg. anion polarizability\"] = np.mean([\n",
    "        average_anion_polarizability[site.specie.element.symbol]\n",
    "        for site in structure.sites\n",
    "        if site.specie.oxi_state < 0\n",
    "    ])\n",
    "\n",
    "    num_anions = len([site for site in structure.sites if site.specie.oxi_state < 0])\n",
    "    num_cations = len([site for site in structure.sites if site.specie.oxi_state >= 0])\n",
    "    struc_dict[\"Condensation\"] = num_anions / num_cations\n",
    "\n",
    "    return struc_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data from Materials Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a349b902fe44a23aa992c716d47306d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Retrieving MaterialsDoc documents:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|‚ñà‚ñà‚ñå       | 1/4 [00:03<00:09,  3.17s/it]No tabulated Shannon radius for La3+ with coordination number 3. Using CN-independent (average) ionic radius instead.\n",
      "No tabulated Shannon radius for Ce3+ with coordination number 3. Using CN-independent (average) ionic radius instead.\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:08<00:00,  2.03s/it]\n"
     ]
    }
   ],
   "source": [
    "initial_df = pd.read_excel(r'Tester.xlsx')\n",
    "initial_formulae = list(initial_df['Composition'])\n",
    "initial_formulae = [\n",
    "    Composition(formula).get_integer_formula_and_factor()[0]\n",
    "    for formula in initial_formulae\n",
    "]\n",
    "\n",
    "materials_fields = [\n",
    "    \"material_id\",\n",
    "    \"formula_pretty\",\n",
    "    \"structure\",\n",
    "    \"symmetry\",\n",
    "    \"volume\",\n",
    "    \"density\"\n",
    "]\n",
    "\n",
    "with MPRester(\"SA8tRgHQuURU1rXg822UJpyaCHNVRuKv\") as mpr:\n",
    "    docs = mpr.materials.search(formula=initial_formulae, fields=materials_fields)\n",
    "\n",
    "Feature_dicts = []\n",
    "\n",
    "for doc in tqdm(docs):\n",
    "    try:\n",
    "        doc.structure.add_oxidation_state_by_guess(max_sites=-1)\n",
    "        sga = SpacegroupAnalyzer(doc.structure)\n",
    "        inequivalent_site_indices = set(sga.get_symmetry_dataset()[\"equivalent_atoms\"])\n",
    "        inequivalent_cation_site_indices = [\n",
    "            i\n",
    "            for i in inequivalent_site_indices\n",
    "            if str(doc.structure.sites[i].specie) in cations_of_interest\n",
    "        ]\n",
    "    \n",
    "        for cation_site_index in inequivalent_cation_site_indices:\n",
    "            cation_specie = doc.structure.sites[cation_site_index].specie\n",
    "\n",
    "            Feature_dicts += [\n",
    "                {\n",
    "                    \"Composition\": str(doc.formula_pretty),\n",
    "                    \"Database IDs\": doc.material_id,\n",
    "                    \"Central Cation\": str(doc.structure.sites[cation_site_index].specie),\n",
    "                    **get_polyhedron_dict(doc.structure, cation_site_index),\n",
    "                    **get_chemenv_info(doc.structure, cation_site_index),\n",
    "                    **get_struc_dict(doc.structure)\n",
    "                }\n",
    "            ]    \n",
    "\n",
    "    except IndexError:\n",
    "        print(f\"skipping composiiton {doc.formula_pretty} due to IndexError\")\n",
    "        continue\n",
    "\n",
    "temp_df = pd.DataFrame(Feature_dicts)\n",
    "temp_df[\"chemenv_CN\"] = temp_df[\"chemenv_CN\"].fillna(temp_df[\"coordination_number\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp_columns = [\n",
    "    'Composition',\n",
    "    'Database IDs',\n",
    "    'Central Cation',\n",
    "    'SGR No.',\n",
    "    'volume_rp',\n",
    "    'density',\n",
    "    'a/b',\n",
    "    'b/c',\n",
    "    'c/a',\n",
    "    'alpha/beta',\n",
    "    'beta/gamma',\n",
    "    'gamma/alpha',\n",
    "    'inversion center',\n",
    "    'polar axis',\n",
    "    'volume_per_Z_rp',\n",
    "    'volume_per_atom'\n",
    "]\n",
    "\n",
    "cs_columns = [\n",
    "    'Composition',\n",
    "    'Database IDs',\n",
    "    'Central Cation',\n",
    "    'coordination_number',\n",
    "    'cation_ionic_radius',\n",
    "    'delta_radius',\n",
    "    'Avg_bond_length',\n",
    "    'Avg. cation electronegativity',\n",
    "    'Avg. anion polarizability',\n",
    "    'Condensation'\n",
    "]\n",
    "\n",
    "d1_columns = [\n",
    "    'Composition',\n",
    "    'Database IDs',\n",
    "    'Central Cation',\n",
    "    'coordination_number',\n",
    "    'cation_ionic_radius',\n",
    "    'dopant_ionic_radius',\n",
    "    'chemenv_CN',\n",
    "    'SGR No.',\n",
    "    'volume',\n",
    "    'volume_per_Z',\n",
    "    'a',\n",
    "    'b',\n",
    "    'gamma'\n",
    "]\n",
    "\n",
    "RP_df = temp_df[rp_columns]\n",
    "CS_df = temp_df[cs_columns]\n",
    "d1_df = temp_df[d1_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data from cif Files for Disordered Structures\n",
    "\n",
    "This part is an alternative for disordered structures or structures not in Materials Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _custom_formatwarning(msg, *args, **kwargs):\n",
    "    return f\"{msg}\\n\"\n",
    "warnings.formatwarning = _custom_formatwarning\n",
    "\n",
    "cif_directory  = os.getcwd()\n",
    "Feature_dicts = []\n",
    "# put the cif files in the same folder where this code exist\n",
    "for cif_filename in os.listdir(cif_directory):\n",
    "    if cif_filename.endswith(\".cif\"):\n",
    "        cif_file_path = os.path.join(cif_directory, cif_filename)\n",
    "\n",
    "        try:\n",
    "            # Parse the CIF file\n",
    "            with open(cif_file_path, \"r\") as cif_file:\n",
    "                cif_parser = CifParser(cif_file)\n",
    "                structure = cif_parser.parse_structures(primitive=True)[0] \n",
    "\n",
    "            odt = OrderDisorderedStructureTransformation()\n",
    "            ordered_structures = odt.apply_transformation(structure, return_ranked_list=True)\n",
    "\n",
    "            if ordered_structures:\n",
    "                structure = ordered_structures[0][\"structure\"]\n",
    "            \n",
    "            structure.add_oxidation_state_by_guess(max_sites=-1)\n",
    "            sga = SpacegroupAnalyzer(structure)\n",
    "            inequivalent_site_indices = set(sga.get_symmetry_dataset()[\"equivalent_atoms\"])\n",
    "            inequivalent_cation_site_indices = [\n",
    "                i\n",
    "                for i in inequivalent_site_indices\n",
    "                if str(structure.sites[i].specie) in cations_of_interest\n",
    "            ]\n",
    "    \n",
    "            for cation_site_index in inequivalent_cation_site_indices:\n",
    "                cation_specie = structure.sites[cation_site_index].specie\n",
    "\n",
    "                Feature_dicts += [\n",
    "                    {\n",
    "                        \"File name\": cif_filename,\n",
    "                        \"Central Cation\": str(structure.sites[cation_site_index].specie),\n",
    "                        **get_polyhedron_dict(structure, cation_site_index),\n",
    "                        **get_chemenv_info(structure, cation_site_index),\n",
    "                        **get_struc_dict(structure)\n",
    "                    }\n",
    "                ]    \n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {cif_filename}: {e}\")\n",
    "\n",
    "temp_df = pd.DataFrame(Feature_dicts)\n",
    "temp_df[\"chemenv_CN\"] = temp_df[\"chemenv_CN\"].fillna(temp_df[\"coordination_number\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Relative Permittivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "permittivity_df = RP_df[[\"Composition\"]]\n",
    "\n",
    "class Vectorize_Formula:\n",
    "    def __init__(self):\n",
    "        elem_dict = pd.read_excel(r'elements_rp.xlsx') # CHECK NAME OF FILE\n",
    "        self.element_df = pd.DataFrame(elem_dict)\n",
    "        self.element_df.set_index('Symbol',inplace=True)\n",
    "        self.column_names = []\n",
    "        for string in ['avg','diff','max','min','std']:\n",
    "            for column_name in list(self.element_df.columns.values):\n",
    "                self.column_names.append(f'{string}_{column_name}')\n",
    "\n",
    "    def get_features(self, formula):\n",
    "        try:\n",
    "            fractional_composition = Composition(formula).fractional_composition.as_dict()\n",
    "            avg_feature = np.zeros(len(self.element_df.columns))\n",
    "\n",
    "            for key in fractional_composition:\n",
    "                if key not in self.element_df.index:\n",
    "                    print('The element:', key, 'from formula', formula,'is not currently supported in our database')\n",
    "                    return np.full(len(self.element_df.colums)*5, np.nan)\n",
    "                avg_feature += self.element_df.loc[key].values * fractional_composition[key]\n",
    "\n",
    "            elements_in_formula = list(fractional_composition.keys())\n",
    "            element_stats = self.element_df.loc[elements_in_formula]\n",
    "            diff_feature = element_stats.max() - element_stats.min()\n",
    "            max_feature = element_stats.max()\n",
    "            min_feature = element_stats.min()\n",
    "            std_feature = element_stats.std(ddof=0)\n",
    "\n",
    "            features = np.concatenate([avg_feature, diff_feature, max_feature, min_feature, std_feature])\n",
    "            return features\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing the formula {formula}: {e}\")\n",
    "            return np.full(len(self.element_df.columns) * 5, np.nan)\n",
    "\n",
    "gf=Vectorize_Formula()\n",
    "\n",
    "# empty list for storage of bnmmbc\n",
    "features=[]\n",
    "\n",
    "# add values to list using for loop\n",
    "for formula in permittivity_df[\"Composition\"]:\n",
    "    features.append(gf.get_features(formula))\n",
    "\n",
    "# feature vectors and targets as X and y\n",
    "X = pd.DataFrame(features, columns = gf.column_names)\n",
    "RP_df_first_three = RP_df.iloc[:, :3]\n",
    "RP_df_rest = RP_df.iloc[:, 3:]\n",
    "RP_Feature_Set=pd.concat([RP_df_first_three, X, RP_df_rest], axis=1)\n",
    "RP_Feature_Set.to_excel('RP_feature_set_for_Tester.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revised Relative Permittivity Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor, plot_importance\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "RPDB = pd.read_excel(r'Training_Set_for_RP.xlsx')\n",
    "array = RPDB.values\n",
    "X = array[:,2:100]\n",
    "Y = array[:,1]\n",
    "Compd = array[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross-validation results for each fold:\n",
      "Fold 1: R2 = 0.9211\n",
      "Fold 2: R2 = 0.8558\n",
      "Fold 3: R2 = 0.9275\n",
      "Fold 4: R2 = 0.8831\n",
      "Fold 5: R2 = 0.8646\n",
      "Fold 6: R2 = 0.8760\n",
      "Fold 7: R2 = 0.8545\n",
      "Fold 8: R2 = 0.8811\n",
      "Fold 9: R2 = 0.8724\n",
      "Fold 10: R2 = 0.8876\n",
      "\n",
      "Mean R2: 0.8824\n",
      "Standard Deviation of R2: 0.0235\n"
     ]
    }
   ],
   "source": [
    "RP_model = XGBRegressor(tree_method='hist', device='cuda',  n_estimators=900, learning_rate=0.06,\n",
    "                          max_depth=5, min_child_weight=9, subsample=0.6, base_score=0.5,\n",
    "                          colsample_bytree=0.5, colsample_bylevel=1, colsample_bynode=1,\n",
    "                          reg_alpha=0, reg_lambda=1\n",
    "                          )\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "cv_results = cross_val_score(RP_model, X, Y, cv=kf, scoring='r2')\n",
    "\n",
    "print(\"\\nCross-validation results for each fold:\")\n",
    "for i, score in enumerate(cv_results, 1):\n",
    "    print(f\"Fold {i}: R2 = {score:.4f}\")\n",
    "\n",
    "print(f\"\\nMean R2: {np.mean(cv_results):.4f}\")\n",
    "print(f\"Standard Deviation of R2: {np.std(cv_results):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "RP_model.fit(X,Y)\n",
    "prediction = pd.read_excel(r'RP_feature_set_for_Tester.xlsx')\n",
    "a = prediction.values\n",
    "b = a[:,3:101]\n",
    "result = RP_model.predict(b)\n",
    "composition=prediction['Composition']\n",
    "result=pd.DataFrame(result, columns=['Predicted RP'])\n",
    "predicted=np.column_stack((composition,result))\n",
    "predicted_RP=pd.DataFrame(predicted)\n",
    "predicted_RP.to_excel('Predicted_RP_for_Tester.xlsx', index=False, header=(\"Composition\",\"Predicted RP\"))\n",
    "\n",
    "CS_df_left = CS_df.iloc[:, :3]\n",
    "CS_df_right = CS_df.iloc[:, 3:]\n",
    "CS_feature_set_df = pd.concat([CS_df_left, result, CS_df_right], axis=1)\n",
    "CS_feature_set_df.to_excel('CS_feature_set_for_Tester.xlsx', index=False)\n",
    "\n",
    "d1_df_left = d1_df.iloc[:, :3]\n",
    "d1_df_right = d1_df.iloc[:, 3:]\n",
    "d1_with_RP_df = pd.concat([d1_df_left, result, d1_df_right], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Centroid Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor, plot_importance\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "CSDB = pd.read_excel('Training_Set_for_CS.xlsx')\n",
    "array = CSDB.values\n",
    "X = array[:,2:10]\n",
    "Y = array[:,1]\n",
    "Compd = array[:,0]\n",
    "\n",
    "#X_train, X_test, Y_train, Y_test = train_test_split (X,Y, test_size=0.2, random_state=22, shuffle=True)\n",
    "#X_tr, X_val, Y_tr, Y_val = train_test_split(X_train, Y_train, test_size=0.1, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross-validation results for each fold:\n",
      "Fold 1: R2 = 0.7404\n",
      "Fold 2: R2 = 0.8558\n",
      "Fold 3: R2 = 0.9627\n",
      "Fold 4: R2 = 0.8985\n",
      "Fold 5: R2 = 0.8846\n",
      "Fold 6: R2 = 0.8847\n",
      "Fold 7: R2 = 0.8875\n",
      "Fold 8: R2 = 0.9672\n",
      "Fold 9: R2 = 0.9373\n",
      "Fold 10: R2 = 0.9276\n",
      "\n",
      "Mean R2: 0.8946\n",
      "Standard Deviation of R2: 0.0619\n"
     ]
    }
   ],
   "source": [
    "CS_model = XGBRegressor(tree_method='hist', device='cuda', n_estimators=150, learning_rate=0.05,\n",
    "                           max_depth=3, min_child_weight=1, subsample=0.6,\n",
    "                           colsample_bytree=0.9, colsample_bylevel=1, colsample_bynode=0.9,\n",
    "                           reg_alpha=0.25, reg_lambda=0)\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=22)\n",
    "cv_results = cross_val_score(CS_model, X, Y, cv=kf, scoring='r2')\n",
    "\n",
    "print(\"\\nCross-validation results for each fold:\")\n",
    "for i, score in enumerate(cv_results, 1):\n",
    "    print(f\"Fold {i}: R2 = {score:.4f}\")\n",
    "\n",
    "print(f\"\\nMean R2: {np.mean(cv_results):.4f}\")\n",
    "print(f\"Standard Deviation of R2: {np.std(cv_results):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "CS_model.fit(X,Y)\n",
    "prediction = pd.read_excel(r'CS_feature_set_for_Tester.xlsx')\n",
    "a = prediction.values\n",
    "b = a[:,3:11]\n",
    "result = CS_model.predict(b)\n",
    "composition=prediction['Composition']\n",
    "result=pd.DataFrame(result, columns=['Predicted CS'])\n",
    "predicted=np.column_stack((composition,result))\n",
    "predicted_CS=pd.DataFrame(predicted)\n",
    "predicted_CS.to_excel('Predicted_CS_for_Tester.xlsx', index=False, header=(\"Composition\",\"Predicted CS\"))\n",
    "\n",
    "d1_with_RP_left = d1_with_RP_df.iloc[:, :3]\n",
    "d1_with_RP_right = d1_with_RP_df.iloc[:, 3:]\n",
    "d1_feature_set_df = pd.concat([d1_with_RP_left, result, d1_with_RP_right], axis=1)\n",
    "d1_feature_set_df.to_excel('d1_wo_compositional_for_Tester.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict 5d1 of Ce3+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "composition_df = pd.read_excel(r\"d1_wo_compositional_for_Tester.xlsx\")\n",
    "\n",
    "class Vectorize_Formula:\n",
    "    def __init__(self):\n",
    "        elem_dict = pd.read_excel(r'elements_5d1.xlsx') # CHECK NAME OF FILE\n",
    "        self.element_df = pd.DataFrame(elem_dict)\n",
    "        self.element_df.set_index('Symbol',inplace=True)\n",
    "        self.column_names = []\n",
    "        for string in ['avg','diff','max','min','std']:\n",
    "            for column_name in list(self.element_df.columns.values):\n",
    "                self.column_names.append(f'{string}_{column_name}')\n",
    "\n",
    "    def get_features(self, formula):\n",
    "        try:\n",
    "            fractional_composition = Composition(formula).fractional_composition.as_dict()\n",
    "            avg_feature = np.zeros(len(self.element_df.columns))\n",
    "\n",
    "            for key in fractional_composition:\n",
    "                if key not in self.element_df.index:\n",
    "                    print('The element:', key, 'from formula', formula,'is not currently supported in our database')\n",
    "                    return np.full(len(self.element_df.colums)*5, np.nan)\n",
    "                avg_feature += self.element_df.loc[key].values * fractional_composition[key]\n",
    "\n",
    "            elements_in_formula = list(fractional_composition.keys())\n",
    "            element_stats = self.element_df.loc[elements_in_formula]\n",
    "            diff_feature = element_stats.max() - element_stats.min()\n",
    "            max_feature = element_stats.max()\n",
    "            min_feature = element_stats.min()\n",
    "            std_feature = element_stats.std(ddof=0)\n",
    "\n",
    "            features = np.concatenate([avg_feature, diff_feature, max_feature, min_feature, std_feature])\n",
    "            return features\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing the formula {formula}: {e}\")\n",
    "            return np.full(len(self.element_df.columns) * 5, np.nan)\n",
    "\n",
    "gf=Vectorize_Formula()\n",
    "\n",
    "# empty list for storage of bnmmbc\n",
    "features=[]\n",
    "\n",
    "# add values to list using for loop\n",
    "for formula in composition_df[\"Composition\"]:\n",
    "    features.append(gf.get_features(formula))\n",
    "\n",
    "# feature vectors and targets as X and y\n",
    "X = pd.DataFrame(features, columns = gf.column_names)\n",
    "columns_to_extract = [1, 3, 4, 5, 7, 8, 9, 10, 12, 13, 15, 23, 24, 29, 30, 32, 34, 40, 52, 53, 66, 72, 74, 76, 78, 79, 80, 83, 84, 87, 89, 93]\n",
    "X_extracted = X.iloc[:, columns_to_extract]\n",
    "completed=pd.concat([composition_df, X_extracted], axis=1)\n",
    "completed.to_excel('5d1_feature_set_for_Tester.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "FDODB=pd.read_excel('Training_Set_for_5d1.xlsx')\n",
    "array = FDODB.values\n",
    "X = array[:,2:46]\n",
    "Y = array[:,1]\n",
    "Compd = array[:,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15338832561753013 0.04344782809517265 0.15425705564280934 0.8380042800908163\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "best_model = XGBRegressor(tree_method='hist', device='cuda', n_estimators=200, learning_rate=0.06,\n",
    "                          max_depth=9, min_child_weight=8, subsample=0.6, base_score=0.4,\n",
    "                          colsample_bytree=1, colsample_bylevel=1, colsample_bynode=1,\n",
    "                          reg_alpha=0, reg_lambda=1\n",
    "                          )\n",
    "\n",
    "logo=LeaveOneGroupOut()\n",
    "# Map group labels to integers\n",
    "unique_groups = np.unique(Compd)\n",
    "group_to_index = {group: idx for idx, group in enumerate(unique_groups)}\n",
    "\n",
    "all_Y_val = []\n",
    "all_Y_pred = []\n",
    "mse_values = []\n",
    "rmse_values = []\n",
    "mae_values = []\n",
    "\n",
    "for train_index, test_index in logo.split(X, Y, Compd):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "    best_model.fit(X_train, Y_train)\n",
    "    Y_pred = best_model.predict(X_test)\n",
    "\n",
    "    mse = mean_squared_error(Y_test, Y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(Y_test, Y_pred)\n",
    "\n",
    "    current_group_label = Compd[test_index][0]\n",
    "\n",
    "    mse_values.append(mse)\n",
    "    rmse_values.append(rmse)\n",
    "    mae_values.append(mae)\n",
    "\n",
    "    all_Y_val.extend(Y_test)\n",
    "    all_Y_pred.extend(Y_pred)\n",
    "\n",
    "avg_mse = np.mean(mse_values)\n",
    "avg_rmse = np.mean(rmse_values)\n",
    "avg_mae = np.mean(mae_values)\n",
    "print(avg_mae, avg_mse, avg_rmse, r2_score(all_Y_val, all_Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.fit(X,Y)\n",
    "prediction = pd.read_excel(r'5d1_feature_set_for_Tester.xlsx')\n",
    "a = prediction.values\n",
    "b = a[:,3:48]\n",
    "result = best_model.predict(b)\n",
    "composition=prediction['Composition']\n",
    "result=pd.DataFrame(result)\n",
    "divider = 1.23984193 * 10**3\n",
    "result_in_nm = result.map(lambda x: round(divider/x, 2))\n",
    "predicted=np.column_stack((composition,result,result_in_nm))\n",
    "predicted=pd.DataFrame(predicted)\n",
    "predicted.to_excel('Predicted_5d1_for_Tester.xlsx', index=False, header=(\"Composition\",\"Predicted 5d1 for Ce3+\",\"Pred 5d1 in nm\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machinelearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
